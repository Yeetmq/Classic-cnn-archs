{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning import Trainer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Datasets.Simpson_dataset import SimpsonDataset\n",
    "\n",
    "from AlexNet.AlexNetArch import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, criterion: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.accuracy_score = Accuracy(task=\"multiclass\", num_classes=42)\n",
    "        self.f1_score = F1Score(task=\"multiclass\", num_classes=42)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        out = self(data)\n",
    "        loss = self.criterion(out, target)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        out = self(data)\n",
    "        val_loss = self.criterion(out, target)\n",
    "\n",
    "        acc = self.accuracy_score(out, target)\n",
    "        f1 = self.f1_score(out, target)\n",
    "        self.log('val_loss', val_loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val_f1', f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters())\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "full_transforms = v2.Compose([v2.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "\n",
    "full_dataset = SimpsonDataset(\n",
    "    path_to_dataset=r'D:\\ethd\\ml\\Classic_archs\\data',\n",
    "    mode='TRAIN',\n",
    "    transforms=train_transforms,\n",
    "    device=device)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "valid_size = len(full_dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "valid_dataset.dataset.transform = val_transforms\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model          | AlexNet            | 58.1 M | train\n",
      "1 | criterion      | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy_score | MulticlassAccuracy | 0      | train\n",
      "3 | f1_score       | MulticlassF1Score  | 0      | train\n",
      "--------------------------------------------------------------\n",
      "58.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "58.1 M    Total params\n",
      "232.252   Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ethd\\conda\\envs\\classic-env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ethd\\conda\\envs\\classic-env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 524/524 [01:55<00:00,  4.53it/s, v_num=0, val_loss=1.810, val_acc=0.538, val_f1=0.538, train_loss=2.010]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 524/524 [02:02<00:00,  4.29it/s, v_num=0, val_loss=1.810, val_acc=0.538, val_f1=0.538, train_loss=2.010]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 10\n",
    "alexnet = AlexNet(in_channel=3, n_classes=42)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = CNN(alexnet, criterion=criterion)\n",
    "\n",
    "csv_logger = CSVLogger(save_dir='logs/', name=\"AlexNet\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=csv_logger,\n",
    "    max_epochs=epoch_num,\n",
    "    accelerator = 'cuda'\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classic-env",
   "language": "python",
   "name": "classic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
